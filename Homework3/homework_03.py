# -*- coding: utf-8 -*-
"""Homework_03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13_3PqOEyFUdLQMJOQFPte9a6iHAav9tR

#Machine Learning Models

##Import Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""##Read Data from CSV"""

#read certain data columns from cses4_cut.csv
dataset = np.array(pd.read_csv('cses4_cut.csv'))
X = dataset[:,1:-1]
N = len(X)

#read the labels column from csv
Y = np.array(pd.read_csv('cses4_cut.csv', usecols=['voted']))
print(X.shape)

## obtain the labels in the form of 1-hot encoding using sklearn
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(sparse=True)
X = encoder.fit_transform(X).toarray() 
Y = encoder.fit_transform(Y).toarray() 
print(X)

"""##Feature Selecetion with Chi Square Method"""

from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest
test = SelectKBest(score_func=chi2, k=6)
fit = test.fit(X, Y)
X =test.fit_transform(X, Y)
fit.scores_

"""##Dimensionality Reduction Using PCA"""

def reduce_dimentions(X):
  from sklearn.decomposition import PCA # 1. Choose the model class
  model = PCA(n_components=2) # 2. Instantiate the model with hyperparameters
  model.fit(X) # 3. Fit to data. Notice y is not specified!
  X = model.transform(X)
  return X

"""##Split Dataset"""

from sklearn.model_selection import train_test_split
X_train, X_test,Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,random_state=1)

# reduce dimensionality with PCA for train and test sets
X_train_r = reduce_dimentions(X_train)
X_test_r= reduce_dimentions(X_test)

"""##Plot Training Dataset"""

plt.scatter(X_train_r, Y_train)
plt.show()

"""##Gaussian Naive Bayes Classifier"""

from sklearn.naive_bayes import GaussianNB # 1. choose model class
model = GaussianNB() # 2. instantiate model
model.fit(X_train, Y_train[:, 1]) # 3. fit model to data
Y_model = model.predict(X_test)

"""##GaussianNB Accuracy Score

"""

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_test[:,1], Y_model)
print(accuracy)

"""##Confusion Matrix for GaussianNB




"""

from sklearn.metrics import confusion_matrix
mat = confusion_matrix(Y_test[:,1], Y_model)
print("The Confusion Matrix for GaussianNB:\n", mat)

"""##KN Neighbors Classifier"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score

def tune_KNN(N, k_values):
  accuracies = []
  for k in k_values:
    # instantiate kNN with given neighbor size k
    knn = KNeighborsClassifier(n_neighbors=k)
    # run cross validation for a given kNN setup
    # I have setup n_jobs=-1 to use all cpus in my env.
    scores = cross_val_score(knn, X_train, Y_train, cv=10, scoring='accuracy', n_jobs=-1)
    accuracies.append(scores.mean())
  return accuracies


k_values = [i for i in range(1, 100)]
accuracies = tune_KNN(N, k_values)
print(accuracies)

model = KNeighborsClassifier(n_neighbors= np.argmax(accuracies))
model.fit(X_train, Y_train)
y_model = model.predict(X_test)
Score = accuracy_score(Y_test, y_model)
print(Score)

plt.plot(k_values, accuracies)
plt.xlabel('Value of K for KNN')
plt.ylabel('Cross-Validated Accuracy')

"""##LogisticRegression Optimization by GridSearchCV




"""

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
param_grid = [
              {'penalty' : ['l1','l2', 'elasticnet'],
               'C' : np.logspace(-8,8,20),
               'solver' : ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],
               'max_iter' : [100,1000, 2500, 5000]
              }]

from sklearn.model_selection import GridSearchCV
clf = GridSearchCV(model, param_grid=param_grid, cv = 3, verbose=True, n_jobs=-1)
clf_best = clf.fit(X_train, Y_train[:,1])

clf_best.best_estimator_

best_score = clf_best.score(X_train, Y_train[:,1])
best_param = clf_best.best_params_

"""##Use of LogisticRegression """

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(random_state=0, C=best_param.get('C'), penalty=best_param.get('penalty'), solver=best_param.get('solver'), max_iter=best_param.get('max_iter'))
model.fit(X_train, Y_train[:,1])
Y_model = model.predict(X_test) 
accuracy = accuracy_score(Y_test[:,1], Y_model)
print(accuracy)